{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BKBKlaassen/Gr8_ModelsForLanguageProcessing_assignments/blob/main/gr8_parsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33320751-6e7a-45bd-881e-9e7175d196bd",
      "metadata": {
        "id": "33320751-6e7a-45bd-881e-9e7175d196bd"
      },
      "source": [
        "# Parsing Assignment (M4LP)\n",
        "\n",
        "The assignment covers dependency parsing. Use a combination of what you learned in class, the instructions, what you already know about coding, and the linked documentation to solve the problems. Please don't use Gemini or other AI tools -- it doesn't work well for this assignment, and will keep you from learning. You can turn them off in Google Colab under Settings.\n",
        "\n",
        "The assignment has 76 points total, and is worth about 4% of your final grade. Components of this assignment will inspire questions on your midterm.\n",
        "\n",
        "Please fill in the cell below with your group number and names and who did what.\n",
        "\n",
        "### Submission\n",
        "\n",
        "You will generate a number of files in this assignment, but you should just submit your version of this Notebook.\n",
        "\n",
        "### Environment setup\n",
        "\n",
        "Run all this code when you start up the notebook to make sure you have everything you need.\n",
        "\n",
        "original by L.abzianidze@uu.nl, updated my m.fowlie@uu.nl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99aa5fc7-fd35-4e0d-840c-c1ad4bf3247e",
      "metadata": {
        "id": "99aa5fc7-fd35-4e0d-840c-c1ad4bf3247e"
      },
      "source": [
        "### Group info\n",
        "\n",
        "Group number: 8\n",
        "\n",
        "Group members: Bjorn Klaassen, Noah de Jonge\n",
        "\n",
        "Who contributed to which exercises (you don't need to be very detailed):\n",
        "\n",
        "exercises done together: Bjorn: , Noah:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b40c7a5-4c30-4704-9cdd-1cb868d8301a",
      "metadata": {
        "id": "5b40c7a5-4c30-4704-9cdd-1cb868d8301a"
      },
      "source": [
        "# Environment setup\n",
        "\n",
        "Run all this code when you start up the notebook to make sure you have everything you need."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61181555-c410-4516-b2d7-0f75e6dba5c7",
      "metadata": {
        "id": "61181555-c410-4516-b2d7-0f75e6dba5c7"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Import spaCy and download its model. Install Stanza that comes with an interface for CoreNLP. Download CoreNLP. Install modules and prepare the environment for rendering syntactic trees of NLTK. Download a course-specific python package that contains useful tools.\n",
        "\n",
        "Additionally, you might find the following predefined function(s) handy: [isinstance](https://www.programiz.com/python-programming/methods/built-in/isinstance), [list comprehension](https://www.programiz.com/python-programming/list-comprehension), [f-string](https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4b96c6-983c-43fc-97d2-869ee6cf55ec",
      "metadata": {
        "id": "ef4b96c6-983c-43fc-97d2-869ee6cf55ec"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "if spacy.__version__ < '3.8':\n",
        "    print(f\"spaCy v={spacy.__version__} but it should be >= 3.8\\nForce install 3.8 with the next cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592021bb-620b-42a5-afcb-4359b174b4ea",
      "metadata": {
        "id": "592021bb-620b-42a5-afcb-4359b174b4ea"
      },
      "outputs": [],
      "source": [
        "# may require environment restart\n",
        "# if necessary, uncomment below and run:\n",
        "# !pip install spacy>=3.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9137c77e-4d09-4af8-af4a-84a790812805",
      "metadata": {
        "id": "9137c77e-4d09-4af8-af4a-84a790812805",
        "outputId": "d8bb5cd3-cea7-488c-b7f3-1f3492de7192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# install medium-sized English model\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31d1d50-8bfd-44bc-a4ef-8df364230e7b",
      "metadata": {
        "id": "a31d1d50-8bfd-44bc-a4ef-8df364230e7b",
        "outputId": "6427d6f9-f1bb-473c-cc49-e593006b2513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
            "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.15.0 stanza-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0065075b-7f3b-4fcb-9cdd-cb14aad9c8e0",
      "metadata": {
        "id": "0065075b-7f3b-4fcb-9cdd-cb14aad9c8e0",
        "outputId": "49804ec8-0b09-4233-f963-a17cc9cb823a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: stanza v=1.11.0. It should be at least 1.5.0\n",
            "If necessary, force install more recent version with the next cell\n"
          ]
        }
      ],
      "source": [
        "import stanza  # may require environment restart\n",
        "# if necessary, uncomment below and run:\n",
        "# !pip install stanza>=1.5.0\n",
        "if stanza.__version__ < '1.5.0':\n",
        "    print(f\"WARNING: stanza v={stanza.__version__}. It should be at least 1.5.0\\nIf necessary, force install more recent version with the next cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f98644b-10cc-41bd-bb93-f6e37b0fede7",
      "metadata": {
        "id": "8f98644b-10cc-41bd-bb93-f6e37b0fede7"
      },
      "outputs": [],
      "source": [
        "# may require environment restart\n",
        "# if necessary, uncomment below and run:\n",
        "# !pip install stanza>=1.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16693b90-a02c-4f21-a3ba-a40c74457e15",
      "metadata": {
        "id": "16693b90-a02c-4f21-a3ba-a40c74457e15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "# Import client module\n",
        "from stanza.server import CoreNLPClient\n",
        "# src: https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb"
      ],
      "metadata": {
        "id": "jEcBRpafDJGO",
        "outputId": "5b2586d1-8474-40b7-c63e-9aeb42dd6c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "f573573c03c746d5a2429ec0464bac1e",
            "e5b0f013afb844e88ab8b81ea2428074",
            "c844633df2e54899aa828c8e34132083",
            "895e8ec7675b4ca9b7402cd2a56f5636",
            "0b4e010d5a6e44f6bc999cb8b0d235de",
            "388abcea4b1848c3956ed0472322e500",
            "aa982f5128384d3ab4427282f49445ae",
            "841a9311e5334a2a9fdbdce904affe72",
            "2694110437234c98b730318eb0473925",
            "00186de3b77d4c7087565c0500e388e6",
            "9d129c057eb94470a42bd0f739f6df26"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Installing CoreNLP package into ./corenlp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/CoreNLP/resolve/main/stanford-corenlp-latest.zip:   0%|        …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f573573c03c746d5a2429ec0464bac1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to ./corenlp/corenlp.zip\n",
            "WARNING:stanza:For customized installation location, please set the `CORENLP_HOME` environment variable to the location of the installation. In Unix, this is done with `export CORENLP_HOME=./corenlp`.\n"
          ]
        }
      ],
      "id": "jEcBRpafDJGO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed to display NLTK's trees objects\n",
        "!pip install svgling"
      ],
      "metadata": {
        "id": "slfQk5ooBVog",
        "outputId": "20b75bc1-549b-4fe0-8033-77bbaa987d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ],
      "id": "slfQk5ooBVog"
    },
    {
      "cell_type": "code",
      "source": [
        "# assigntools package is a course specific collection of useful tools\n",
        "! rm -rf assigntools\n",
        "! git clone https://github.com/megodoonch/assigntools.git"
      ],
      "metadata": {
        "id": "kPIBvjxg9aN8",
        "outputId": "2cf1efb9-39cd-4f3c-d5fc-d3700cb84c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assigntools'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 328 (delta 68), reused 57 (delta 28), pack-reused 200 (from 1)\u001b[K\n",
            "Receiving objects: 100% (328/328), 3.24 MiB | 8.80 MiB/s, done.\n",
            "Resolving deltas: 100% (170/170), done.\n"
          ]
        }
      ],
      "id": "kPIBvjxg9aN8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "ziLWG53kfGST"
      },
      "id": "ziLWG53kfGST"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os, sys\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "from spacy import displacy\n",
        "import importlib\n",
        "from typing import List, Iterable\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "JDIzxmBYxQV8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JDIzxmBYxQV8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Course-specific package\n",
        "from assigntools.M4LP.A1 import read_pickle, write_pickle, download_extract_zip, flatten_list, display_doc_dep"
      ],
      "metadata": {
        "id": "glcaegh-yHNg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "glcaegh-yHNg"
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "print(f\"spaCy version: {spacy.__version__}\")    # should be >= 3.8\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NLTK version: {nltk.__version__}\")\n",
        "print(f\"stanza version: {stanza.__version__}\") # should be >= 1.5.0"
      ],
      "metadata": {
        "id": "P3JBRKtzxSRT",
        "outputId": "4f0dc5c1-9ded-483b-eaee-e68d1a053fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy version: 3.8.11\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "NLTK version: 3.9.1\n",
            "stanza version: 1.11.0\n"
          ]
        }
      ],
      "id": "P3JBRKtzxSRT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "OLrqZ_pK0Of4"
      },
      "id": "OLrqZ_pK0Of4"
    },
    {
      "cell_type": "code",
      "source": [
        "# read-only\n",
        "# URL of a file that will be used during the assignmnet\n",
        "SICK_TRIAL_URL =  \"http://alt.qcri.org/semeval2014/task1/data/uploads/sick_trial.zip\"\n",
        "files = download_extract_zip(SICK_TRIAL_URL)"
      ],
      "metadata": {
        "id": "BHUPuYpr0T7Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BHUPuYpr0T7Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Students' additional modules\n",
        "\n",
        "If you need any additional modules, import them here."
      ],
      "metadata": {
        "id": "HcD8C5xA1isO"
      },
      "id": "HcD8C5xA1isO"
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT ALL ADDED AND NECESSARY MODULES HERE (IF ANY)\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pTz2S1M61vAj"
      },
      "execution_count": 34,
      "outputs": [],
      "id": "pTz2S1M61vAj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex1[10pts]: Extracting sentences\n",
        "\n",
        "Often when parsing a bunch of sentences, it is a good practice to parse each sentence only once and decrease the parsing time. The number of sentences in this exercises are not too much, so saved parsing time in the end will be ~2-3min, but sometimes in real applications such tricks can save hours.\n",
        "\n",
        "Complete the following function so that it does what is says in the docstring. Make sure the \"...\" is removed and that you include comments that explain your code. Feel free to update the docstring as well if you like.\n",
        "\n",
        "The file the function is supposed to read is tab-seperated-value file. You can use string operations or regex to read the sentences but the best practice is to use ready modules that provide file readers for common file formats.\n",
        "\n",
        "`SICK_trial.txt` has the format your function needs to be able to extract sentences from. Extract all sentences.\n",
        "\n"
      ],
      "metadata": {
        "id": "cxE9qimJxZMh"
      },
      "id": "cxE9qimJxZMh"
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.sas.sasreader import FilePath\n",
        "################################################################################\n",
        "################################## EXERCISE 1 ##################################\n",
        "################################################################################\n",
        "\n",
        "def read_tsv_sentences(file_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Takes the path of a tab-seperated-value file and reads all sentences from it\n",
        "    File is formatted as in SICK_trial.txt.\n",
        "    Return a list of str (the sentences) that is sorted (in ascending order)\n",
        "        and duplicate ones are filtered out.\n",
        "    \"\"\"\n",
        "    file_list = pd.read_csv(file_path, sep='\\t')\n",
        "    return file_list;\n",
        "\n"
      ],
      "metadata": {
        "id": "A7JcRJ38LLHq",
        "outputId": "a84be8bc-52c5-4c01-ee8e-81c7e11d660f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "id": "A7JcRJ38LLHq",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'FilePath' from 'pandas.io.sas.sasreader' (/usr/local/lib/python3.12/dist-packages/pandas/io/sas/sasreader.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1049579736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msasreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFilePath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m################################## EXERCISE 1 ##################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FilePath' from 'pandas.io.sas.sasreader' (/usr/local/lib/python3.12/dist-packages/pandas/io/sas/sasreader.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A toy data set: 3 sentences from the SICK trial. Useful for testing function behaviour quickly, and even if EX1 isn't done yet.\n",
        "\n",
        "toy_sick = ['A baby is playing with a doll', 'A baby is playing with a toy', 'A baby tiger is playing with a ball']"
      ],
      "metadata": {
        "id": "slXQKx-CHYGh"
      },
      "execution_count": 20,
      "outputs": [],
      "id": "slXQKx-CHYGh"
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX1\n",
        "sents = read_tsv_sentences('SICK_trial.txt')\n",
        "print(sents[0])\n",
        "assert sents[:3] == toy_sick, f\"first 3 sentences are incorrect\""
      ],
      "metadata": {
        "id": "UPPcskQqEbDe",
        "outputId": "6867ef03-44b3-4a89-ad56-6c75e2a99e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pair_ID', 'sentence_A', 'sentence_B', 'relatedness_score', 'entailment_judgment']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "first 3 sentences are incorrect",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-650311118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_tsv_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SICK_trial.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoy_sick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"first 3 sentences are incorrect\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: first 3 sentences are incorrect"
          ]
        }
      ],
      "id": "UPPcskQqEbDe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex2[10pt] and Ex3[1pt]: Parsing and tagging with spaCy\n",
        "\n",
        "Now it is time to parse sentences. We will use [spaCy](https://spacy.io/) for getting dependency trees of the sentences. In addition to the dependency parsing, spaCy pipeline also does part-of-speech tagging and lemmatization (with other stuff). In this exercise, we write and read spaCy parses to and from CONLL files.\n",
        "\n",
        "For a quick intro to spaCy, have a look at the following section in the [spaCy tutorial](https://course.spacy.io/en/): sections 1 and 5 in [chapter 1](https://course.spacy.io/en/chapter1), and 4 in [chapter 2](https://course.spacy.io/en/chapter2).\n",
        "Use attributes of spaCy's [Token objects](https://spacy.io/api/token).\n",
        "After annotation, tokens come with two pos tags: fine-grained corresponds to [Penn Treebank pos tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) while coarse-grained to [Universal pos tags](https://universaldependencies.org/u/pos/).\n",
        "\n",
        "A CONLL file is a common format for dependencies. Take a look at [the format for Universal Dependencies](https://universaldependencies.org/format.html) as an example. In this exercise, you'll familiarise yourself with the SpaCy `Doc` object that the parser generates by writing a list of them to a CONLL file with a particular format, as given in the doc string. You can also see exactly what it should write for a toy corpus, below.\n",
        "\n",
        "Ex2: Complete the `spacy2conll` function so that it does what it says in the docstring. Ex3: Test it by writing the toy dataset below, `toy_parsed`, to file. Manually inspect the file `toy_spacy_sm.conll`. It should look like this:"
      ],
      "metadata": {
        "id": "puBKCjT4xTsD"
      },
      "id": "puBKCjT4xTsD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# sent_id = 1\n",
        "text = A baby is playing with a doll\n",
        "0\tA\ta\tDET\tDT\t1\tdet\n",
        "1\tbaby\tbaby\tNOUN\tNN\t3\tnsubj\n",
        "2\tis\tbe\tAUX\tVBZ\t3\taux\n",
        "3\tplaying\tplay\tVERB\tVBG\t3\tROOT\n",
        "4\twith\twith\tADP\tIN\t3\tprep\n",
        "5\ta\ta\tDET\tDT\t6\tdet\n",
        "6\tdoll\tdoll\tNOUN\tNN\t4\tpobj\n",
        "\n",
        "# sent_id = 2\n",
        "text = A baby is playing with a toy\n",
        "0\tA\ta\tDET\tDT\t1\tdet\n",
        "1\tbaby\tbaby\tNOUN\tNN\t3\tnsubj\n",
        "2\tis\tbe\tAUX\tVBZ\t3\taux\n",
        "3\tplaying\tplay\tVERB\tVBG\t3\tROOT\n",
        "4\twith\twith\tADP\tIN\t3\tprep\n",
        "5\ta\ta\tDET\tDT\t6\tdet\n",
        "6\ttoy\ttoy\tNOUN\tNN\t4\tpobj\n",
        "\n",
        "# sent_id = 3\n",
        "text = A baby tiger is playing with a ball\n",
        "0\tA\ta\tDET\tDT\t2\tdet\n",
        "1\tbaby\tbaby\tNOUN\tNN\t2\tcompound\n",
        "2\ttiger\ttiger\tNOUN\tNN\t4\tnsubj\n",
        "3\tis\tbe\tAUX\tVBZ\t4\taux\n",
        "4\tplaying\tplay\tVERB\tVBG\t4\tROOT\n",
        "5\twith\twith\tADP\tIN\t4\tprep\n",
        "6\ta\ta\tDET\tDT\t7\tdet\n",
        "7\tball\tball\tNOUN\tNN\t5\tpobj\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "hJ0Q3E_9oU72"
      },
      "id": "hJ0Q3E_9oU72"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse and display a corpus"
      ],
      "metadata": {
        "id": "-LstVaEqt687"
      },
      "id": "-LstVaEqt687"
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing sentences with spaCy's small model\n",
        "\n",
        "# load the small model\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "LSinzLPdyMb_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LSinzLPdyMb_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the toy dataset\n",
        "toy_parsed = nlp_sm.pipe(toy_sick)\n",
        "# convert it to a list\n",
        "toy_parsed = list(toy_parsed)"
      ],
      "metadata": {
        "id": "LB64n6-8IjBy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LB64n6-8IjBy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display spaCy's dependency trees with the help of displaCy\n",
        "for parse in toy_parsed:\n",
        "    display_doc_dep(parse)"
      ],
      "metadata": {
        "id": "sfoEmZo3tlfg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sfoEmZo3tlfg"
    },
    {
      "cell_type": "code",
      "source": [
        "# we can regulate space between tokens, but it might affect readability of labels\n",
        "display_doc_dep(toy_parsed[0], d=100)"
      ],
      "metadata": {
        "id": "cM8SJGZFuZrx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cM8SJGZFuZrx"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 2 ##################################\n",
        "################################################################################\n",
        "\n",
        "def spacy2conll(parsed_sentences, out_path: str):\n",
        "    \"\"\"\n",
        "    Given an iterable over SpaCy parses (e.g. a list), write a CONLL file,\n",
        "    formatted as follows:\n",
        "    Elements are tab-separated.\n",
        "    Word indices are 0-indexed.\n",
        "    Sentence IDs are the 1-indexed indices from the input\n",
        "    Add a newline between sentences.\n",
        "    Format:\n",
        "    # sent id n\n",
        "    # text = This is the sentence.\n",
        "    0 word lemma coarse_pos fine_pos head_index dep_label\n",
        "    1 word ...\n",
        "    \"\"\"\n",
        "    ..."
      ],
      "metadata": {
        "id": "ieLOS3xILxGg"
      },
      "id": "ieLOS3xILxGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 3 ##################################\n",
        "################################################################################\n",
        "\n",
        "# Use spacy2conll to write the parsed toy corpus to toy_spacy_sm.conll\n"
      ],
      "metadata": {
        "id": "5AqoexuUL1WI"
      },
      "id": "5AqoexuUL1WI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex4 [10pts] and Ex5 [1pt]\n",
        "\n",
        "Now reverse the process. Write a function to read in a file like the one generated in Ex 2 above, and return a list of `spacy.token.Doc` objects.\n",
        "\n",
        "A few notes:\n",
        "\n",
        "* The `vocab` argument is needed to initialise a `Doc`. When you run this function, use the `vocab` from the model used to parse it. For the small model loaded in this Notebook, that's `nlp_sm.vocab`.\n",
        "* There's a bug in `Doc.__init__` that adds an extra space at the end of `Doc.text` when you create it directly with the `__init__` function instead of by parsing with the model. To help you check your work, we provide `check_doc_equality` that checks all attributes that are included in the CONLL file.\n",
        "* You'll probably find the [documentation](https://spacy.io/api/doc) for `Doc` helpful\n",
        "\n",
        "Then test your function by reading back in your conll file and checking the Docs against the original parses (Ex5)."
      ],
      "metadata": {
        "id": "ksC9L6A3pWPE"
      },
      "id": "ksC9L6A3pWPE"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 4 ##################################\n",
        "################################################################################\n",
        "\n",
        "def conll2spacy(input_path, vocab):\n",
        "    \"\"\"\n",
        "    Given a path to a CONLL file and a SpaCy Vocab object,\n",
        "    read in the conll file and return a list of SpaCy Docs\n",
        "     containing the same information.\n",
        "    @param input_path: str: path to conll file\n",
        "    @vocab spaCy Vocab object: the vocab of the model used to parse the sentence\n",
        "    @return: list of Docs\n",
        "    \"\"\"\n",
        "    ..."
      ],
      "metadata": {
        "id": "v4ixX93qL9gG"
      },
      "id": "v4ixX93qL9gG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provided\n",
        "\n",
        "def check_doc_equality(doc1, doc2):\n",
        "    \"\"\"\n",
        "    A bug in the SpaCy Doc initaliser adds a space to the end of the\n",
        "     doc.text when built from components, rather than from the parser.\n",
        "    To get around it, check equality of Doc objects with this function.\n",
        "    \"\"\"\n",
        "    if len(doc1) != len(doc2):\n",
        "        return False\n",
        "    for token1, token2 in zip(doc1, doc2):\n",
        "        if token1.text != token2.text: return False\n",
        "        if token1.lemma != token2.lemma: return False\n",
        "        if token1.tag != token2.tag: return False\n",
        "        if token1.pos != token2.pos: return False\n",
        "        if token1.head.i != token2.head.i: return False\n",
        "        if token1.dep != token2.dep: return False\n",
        "        return True"
      ],
      "metadata": {
        "id": "U_g6KVOHmzjI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "U_g6KVOHmzjI"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 5 ##################################\n",
        "################################################################################\n",
        "\n",
        "# Read in the conll file you wrote and check whether each of the 3 entries\n",
        "# is correct. Use check_doc_equality.\n",
        "\n",
        "# Display the original and re-read parses with display_doc_dep\n",
        "# and visually inspect them"
      ],
      "metadata": {
        "id": "UMNfJZpsMJ8i"
      },
      "id": "UMNfJZpsMJ8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex6 [3pts]: Parse the real corpus\n",
        "\n",
        "Perform the following tasks. Print messages to the screen as appropriate.\n",
        "\n",
        "1. Parse the full SICK trial corpus with the `nlp_sm` model\n",
        "2. Write them to a conll file `full_spacy_sm.conll`\n",
        "2. Read them back in\n",
        "2. Check the re-read-in `Doc`s are the same as the originals\n",
        "2. Visually inspect three parses (not the first three)\n",
        "2. Parse the full corpus with the medium English model (see next cell)\n",
        "2. Write the medium-parsed corpus to `full_spacy_md.conll`\n",
        "\n",
        "Note: Because this is a small corpus and a fast parser, you can choose, when you need the parsed corpus later, whether to read it in from file or just use what's in the memory from the parsing earlier in the Notebook. If you're finding any of this slow, you may want to download the conll files to your own device, so you don't have to re-parse in future."
      ],
      "metadata": {
        "id": "Souwvu6PvTWC"
      },
      "id": "Souwvu6PvTWC"
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing all sentences with spaCy's medium model\n",
        "\n",
        "# load the medium model\n",
        "nlp_md = spacy.load(\"en_core_web_md\")\n"
      ],
      "metadata": {
        "id": "KIZ2bJkHyJBc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KIZ2bJkHyJBc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex7[10pt] and Ex8[1pt]: Projectivity\n",
        "\n",
        "Use spaCy's [Token attributes or methods](https://spacy.io/api/token) related to dependency annotations. This will make code much much simpler.\n",
        "\n",
        "As you learned in class, not all dependency trees are projective (see example below). Although spaCy uses an Arc-Eager algorithm, [it has some additional functionality](https://spacy.io/api/dependencyparser/) which makes non-projective parses possible as well.\n",
        "\n",
        "Use the definition in the lecture (and text book) to complete `is_projective` so that it checks the projectivity of a given spaCy `Doc`.\n",
        "\n",
        "Sentence 689, at least in spaCy 3.8.11, has a non-projective parse from the small model and a projective parse from the medium model. To get a look at a non-project and projective tree, you can use `display_doc_dep`. You may want the optional argument `compact=False` to make curved arcs."
      ],
      "metadata": {
        "id": "iypVScvA8gmk"
      },
      "id": "iypVScvA8gmk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example projective and non-projective trees\n",
        "# using compact=False to better display crossing arcs\n",
        "\n",
        "\n",
        "display_doc_dep(..., d=100, compact=False)  # sm\n",
        "display_doc_dep(..., d=100, compact=False)  # md"
      ],
      "metadata": {
        "id": "x0NwzlTtnZYq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "x0NwzlTtnZYq"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 7 ##################################\n",
        "################################################################################\n",
        "\n",
        "def is_projective(doc):\n",
        "    \"\"\"\n",
        "    Checks a dependency tree on projectivity. Uses the definition\n",
        "        of projective arcs and checks all arcs on projectivity.\n",
        "    @param doc: spaCy Doc object\n",
        "    @return Bool (True if projective)\n",
        "    \"\"\"\n",
        "    ..."
      ],
      "metadata": {
        "id": "goDhWDgrOE5R"
      },
      "id": "goDhWDgrOE5R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 8 ##################################\n",
        "################################################################################\n",
        "\n",
        "# Test on a projective and non-projective parse\n"
      ],
      "metadata": {
        "id": "hWdFtMmqOIg0"
      },
      "id": "hWdFtMmqOIg0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST on all parses from small model\n",
        "for i, d in enumerate(docs_sm):\n",
        "    if not is_projective(d):\n",
        "        print(f\"{i}: {d}\")\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "0_g9jewVOMxU"
      },
      "id": "0_g9jewVOMxU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST on all parses from medium model\n",
        "for i, d in enumerate(docs_md):\n",
        "    if not is_projective(d):\n",
        "        print(f\"{i}: {d}\")\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "N-mmXJTLOOGC"
      },
      "id": "N-mmXJTLOOGC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex9 [5pts]\n",
        "\n",
        "The following sentence should have a non-projective parse, but the even the medium model for SpaCy version 3.8.11 does not predict a non-projective parse. (The model in version 3.7.1 did for some reason!)\n",
        "\n",
        "_Who did you say Mary likes?_\n",
        "\n",
        "Parse this sentence with both SpaCy models. Use your function to check them for projectivity. Display them with `display_doc_dep`.\n",
        "\n",
        "Write in a text (Markdown) cell a short discussion: If you get a non-projective parse, which model yields it, and what edge is non-projective? If not, what edge SHOULD be non-projective? What mistake is the parser making?\n",
        "\n",
        "Your answer should be code and markdown cells that parse, display, explain, etc."
      ],
      "metadata": {
        "id": "BEjUYp7u6if7"
      },
      "id": "BEjUYp7u6if7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing with CoreNLP\n",
        "\n",
        "Another library for parsing into dependency trees is CoreNLP.\n",
        "\n",
        "CoreNLP will be used through [Stanza CoreNLP interface](https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb). CoreNLP provides both constituency and dependency trees."
      ],
      "metadata": {
        "id": "HM1sj-8BpFtV"
      },
      "id": "HM1sj-8BpFtV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dependency trees from a dependency parser\n",
        "# takes <1min\n",
        "# https://stanfordnlp.github.io/CoreNLP/depparse.html\n",
        "\n",
        "with CoreNLPClient(annotators='tokenize,pos,depparse',\n",
        "                   memory='4G', endpoint='http://localhost:9021', be_quiet=True,\n",
        "                   output_format='json') as client:\n",
        "    core_dep_parses = [ client.annotate(s)['sentences'][0] for s in sents ]"
      ],
      "metadata": {
        "id": "MtH9IG39p4jO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MtH9IG39p4jO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex10[10pts], Ex11[1pt], Ex12[1pt]: From CoreNLP to Doc\n",
        "\n",
        "CoreNLP dependencies, e.g., `core_dep_parses[0]['basicDependencies']` are a list of dictionaries each corresponding to a token. For trees, it would be handy if the dependencies are formatted as spaCy's [Doc object](https://spacy.io/api/doc), which allows us to display dependency trees (or check projectivity). Read how [Doc](https://spacy.io/api/doc) can be initialized. You should find `core_dep_parses[0]['tokens']` useful for getting values of `spaces` and `tags` arguments.\n",
        "\n",
        "Ex11: Test your function by displaying the first three parses along with the parses from the other parser for the same sentences. Inspect them and see how they are similar and different.\n",
        "\n",
        "Ex12: Convert all CoreNLP parses to SpaCy Doc notation and store them in a variable. Write them to a conll file, `full_stanza.conll`. Check their projectivity."
      ],
      "metadata": {
        "id": "k3ECTHM0Lsla"
      },
      "id": "k3ECTHM0Lsla"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 10 #################################\n",
        "################################################################################\n",
        "\n",
        "def coreNLP2Doc(parse):\n",
        "    \"\"\"\n",
        "    Uses info from parse['basicDependencies'] and parse['tokens']\n",
        "        to initialize and return a Doc object.\n",
        "    @param parse: a parse from CoreNLP\n",
        "    @return: spacy.tokens.Doc with the basicDependencies, including tags.\n",
        "    \"\"\"\n",
        "    # initialise a vocabulary for spacy Doc\n",
        "    vocab = spacy.Vocab()\n",
        "\n",
        "    ..."
      ],
      "metadata": {
        "id": "N53kXdWSOcxD"
      },
      "id": "N53kXdWSOcxD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 11 #################################\n",
        "################################################################################\n",
        "\n",
        "# Test your function"
      ],
      "metadata": {
        "id": "cgKJTXFvAaJs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cgKJTXFvAaJs"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 12 #################################\n",
        "################################################################################\n",
        "\n",
        "# parse all SICK trial items and write them full_stanza.conll\n",
        "\n",
        "core_dep_parses = ..."
      ],
      "metadata": {
        "id": "gZkTYukYAeIo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gZkTYukYAeIo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced dependency graphs\n",
        "\n",
        "In Lecture 4 you learned about Enhanced Dependencies. These are also provided in the Core NLP parses, under the key `'enhancedDependencies'`.\n"
      ],
      "metadata": {
        "id": "C_98uKXa9TKr"
      },
      "id": "C_98uKXa9TKr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex 13 [10ts]: Generate Dot file\n",
        "\n",
        "You are provided the function `write_to_dot` that takes a dict form of a graph and writes it as a dot file.\n",
        "\n",
        "Your job is to complete the function below it, `graph2dictionary`, to create the dict that is input to `write_to_dot`. Use the docstrings of both functions and the code in `write_to_dot` to guide how you build the dict in your function.\n",
        "\n",
        "The tests below will write a dot file, which you can inspect, as well as download and run Dot on, if you have that working, so that you can view the graph. The file should appear to the left of this Notebook (11.dot) and right-clicking should allow you to download it, and generate an image using `dot`. You can also change the syntax very slightly and put it in a LaTeX file -- see HW1.\n",
        "\n",
        "You don't need to include the output files in your submission."
      ],
      "metadata": {
        "id": "i1ADyKfa7zyK"
      },
      "id": "i1ADyKfa7zyK"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROVIDED\n",
        "\n",
        "def write_to_dot(dictionary, dot_file_path):\n",
        "    \"\"\"\n",
        "    Given a graph in dictionary form, write Dot code to a file.\n",
        "    Dict should be in the form output by graph2dict.\n",
        "\n",
        "    @param dictionary:\n",
        "        head (int): dict:\n",
        "                        \"label\": node label,\n",
        "                        \"deps\": dict:\n",
        "                            dependent (int): edge label\n",
        "    @param dot_file_path (str): path to write code to (including filename.dot)\n",
        "    \"\"\"\n",
        "    with open(dot_file_path, 'w') as dot:\n",
        "        dot.write(\"digraph g {\\n\")\n",
        "        for node in dictionary:\n",
        "            # write the node and its label\n",
        "            dot.write(f\"{node} [label=\\\"{dictionary[node]['label']}\\\"];\\n\")\n",
        "            for (dep, label) in dictionary[node][\"deps\"].items():\n",
        "                # write the edge and its label\n",
        "                dot.write(f\"{node} -> {dep} [label=\\\"{label}\\\"];\\n\")\n",
        "        dot.write(\"}\")"
      ],
      "metadata": {
        "id": "5vs3t4ndl5y8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5vs3t4ndl5y8"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 13 #################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def graph2dictionary(edges):\n",
        "    \"\"\"\n",
        "    Given a dependency list from one parsed sentence from CoreNLP,\n",
        "    e.g. the 'enhancedDependencies' entry,\n",
        "    extracts the nodes, edges, and labels into a dict.\n",
        "    Returns the dictionary.\n",
        "\n",
        "    @param edges: a list of edges from an entry from CoreNLP (type dict)\n",
        "    @return dict\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "Tp58rc48OlXW"
      },
      "id": "Tp58rc48OlXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST run the function on entry 11's enhanced dependencies\n",
        "graph_11 = graph2dictionary(core_dep_parses[11]['enhancedDependencies'])"
      ],
      "metadata": {
        "id": "pmzH1Zt-u0Fr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pmzH1Zt-u0Fr"
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST write to dot file\n",
        "write_to_dot(graph_11, \"11.dot\")\n"
      ],
      "metadata": {
        "id": "zQoA8yeBrvZm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zQoA8yeBrvZm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex 14 [3pts]\n",
        "\n",
        "The CONLL file format we have been using will not work for Enhanced Dependencies. Why not?"
      ],
      "metadata": {
        "id": "ukcck3OpQ7tm"
      },
      "id": "ukcck3OpQ7tm"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QUWRIhAiRRbY"
      },
      "id": "QUWRIhAiRRbY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f573573c03c746d5a2429ec0464bac1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5b0f013afb844e88ab8b81ea2428074",
              "IPY_MODEL_c844633df2e54899aa828c8e34132083",
              "IPY_MODEL_895e8ec7675b4ca9b7402cd2a56f5636"
            ],
            "layout": "IPY_MODEL_0b4e010d5a6e44f6bc999cb8b0d235de"
          }
        },
        "e5b0f013afb844e88ab8b81ea2428074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_388abcea4b1848c3956ed0472322e500",
            "placeholder": "​",
            "style": "IPY_MODEL_aa982f5128384d3ab4427282f49445ae",
            "value": "Downloading https://huggingface.co/stanfordnlp/CoreNLP/resolve/main/stanford-corenlp-latest.zip: 100%"
          }
        },
        "c844633df2e54899aa828c8e34132083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841a9311e5334a2a9fdbdce904affe72",
            "max": 508444875,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2694110437234c98b730318eb0473925",
            "value": 508444875
          }
        },
        "895e8ec7675b4ca9b7402cd2a56f5636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00186de3b77d4c7087565c0500e388e6",
            "placeholder": "​",
            "style": "IPY_MODEL_9d129c057eb94470a42bd0f739f6df26",
            "value": " 508M/508M [00:07&lt;00:00, 70.6MB/s]"
          }
        },
        "0b4e010d5a6e44f6bc999cb8b0d235de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "388abcea4b1848c3956ed0472322e500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa982f5128384d3ab4427282f49445ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "841a9311e5334a2a9fdbdce904affe72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2694110437234c98b730318eb0473925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00186de3b77d4c7087565c0500e388e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d129c057eb94470a42bd0f739f6df26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}