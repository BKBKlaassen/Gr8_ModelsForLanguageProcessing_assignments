{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BKBKlaassen/Gr8_ModelsForLanguageProcessing_assignments/blob/main/Group_8_Assignment_4_2026_student_b_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zydk5m0FpaKT"
      },
      "source": [
        "## Assignment 4\n",
        "\n",
        "In this assignment, you are asked to work with pretrained language models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "Group number: 8\n",
        "\n",
        "Group members: Bjorn Klaassen, Noah de Jonge\n",
        "\n",
        "Who contributed to which exercises (you don't need to be very detailed):"
      ],
      "metadata": {
        "id": "8rvH77dcj-2Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ledaEclUpvzT"
      },
      "source": [
        "In this assignment, you will work with pretrained language models. While they are small by modern standards, the computation required in the exercises below can be sped up if you use GPU for it. In the menu on Colab, go to \"Runtime > Change runtime type\" to enable GPU device (designated as `'cuda'` in the code).\n",
        "\n",
        "Google Colab offers limited GPU resources, which are sufficient for doing the entire assignment. However, if you debug your code by repeatedly running large amounts of GPU computation, you may run out of the GPU allocation provided by Google. One thing you should learn from this assignment is to use GPU compute strategically, debugging without a GPU or with reduced data or computation. (GPU resources are just as scarce in the 'real world', outside of the simple tasks we do in class: for example, there are open weight language models with advanced capabilities, such as DeepSeek-R1, that theoretically anyone can run, but in practice the hardware requirements are prohibitive.)\n",
        "\n",
        "If you do run out of GPU allocation on Colab, you can usually continue working in a different Google account as a workaround.\n",
        "\n",
        "To start the assignment, import prerequisite packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRh6vltQpvTn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import nltk,sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM9Qv40viw_l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import collections, itertools\n",
        "import more_itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.1 BERT-like Model for Classification"
      ],
      "metadata": {
        "id": "hJh9S94-N_D1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dvw-UmFNO1Z"
      },
      "source": [
        "##4.1.1 SICK dataset\n",
        "\n",
        "In Assignment 2, we did entailment (hypernymy) classification on the basis of word vectors. Now we can do a similar experiment for sentence embeddings. Start by downloading the SICK dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3_ak3ugNNqZ"
      },
      "outputs": [],
      "source": [
        "!wget https://zenodo.org/record/2787612/files/SICK.zip?download=1 -O SICK.zip\n",
        "!unzip SICK.zip\n",
        "!rm SICK.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p8hLMOjN4AI"
      },
      "outputs": [],
      "source": [
        "!head SICK.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymPQ1m3yOSJg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "sick_df = pd.read_csv('SICK.txt', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can inspect the first few data entries of the SICK dataset. You will see sentence A and sentence B and the entailment_labels, indicating whether sentence A entails sentence B."
      ],
      "metadata": {
        "id": "8D5_KfPsLPER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sick_df"
      ],
      "metadata": {
        "id": "ZUrGFDUXLIpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibP-oQUPLds5"
      },
      "source": [
        "Read the train and test data from the file. Be sure to include in the training data all sentence pairs marked as \"train\" or \"trial\" in the SICK.txt file, and in the test data all sentence pairs marked as \"test\". As labels, use values from the `entailment_label` column in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Xs-Op5MARu"
      },
      "outputs": [],
      "source": [
        "sick_train_examples=[]\n",
        "sick_test_examples=[]\n",
        "sick_train_labels=[]\n",
        "sick_test_labels=[]\n",
        "\n",
        "for i in range(len(sick_df)):\n",
        "  if(sick_df.SemEval_set[i] == \"TRAIN\" or sick_df.SemEval_set[i] == \"TRIAL\"):\n",
        "    sick_train_examples.append((sick_df.sentence_A[i],sick_df.sentence_B[i]))\n",
        "    sick_train_labels.append(sick_df.entailment_label[i])\n",
        "  elif(sick_df.SemEval_set[i] == \"TEST\"):\n",
        "    sick_test_examples.append((sick_df.sentence_A[i],sick_df.sentence_B[i]))\n",
        "    sick_test_labels.append(sick_df.entailment_label[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how many examples and label you have got in each partition:"
      ],
      "metadata": {
        "id": "RJLeZDgD44Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sick_train_examples),len(sick_train_labels),len(sick_test_examples),len(sick_test_labels))"
      ],
      "metadata": {
        "id": "rXO8jKhP4pBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtfHcBiTMJ5u"
      },
      "source": [
        "##4.1.2. Use a pretrained language model for sequence embedding\n",
        "\n",
        "We can rely here on Huggingface which provides many pretrained models in its ```transformers``` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p1UrqEzPC93"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjYHth0_TlRR"
      },
      "source": [
        "Here we can use a relatively small BERT-like model called DistilBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU3yssk9OzAx"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfU2rXewQRUY"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\",padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy1BmIpBQXVn"
      },
      "outputs": [],
      "source": [
        "distilbert = transformers.AutoModel.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGVTP6HrUOy0"
      },
      "source": [
        "Tokeniser of the model does a lot of heavy lifting. It takes in a sentence or a pair of sentences, concatenates them, tokenizes, adds [CLS] token (id: 101) and separator(s) [SEP] token (id: 102) and returns a nensor with the list of token ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZY0g2nqQoqH"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(sick_df.sentence_A.tolist()[5],sick_df.sentence_B.tolist()[5], return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Pxg96gQumI"
      },
      "outputs": [],
      "source": [
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many tokens does the tokenizer recognise in sentences A and B in the example above?\n",
        "\n",
        "__Answer:__\n",
        "\n",
        "Not including the CLS token,\n",
        "\n",
        "Sentence A: 13 tokens\n",
        "\n",
        "Sentence B: 19 tokens\n",
        "\n",
        "Together: 32 tokens\n",
        "\n",
        "Together + CLS token: 33 Tokens"
      ],
      "metadata": {
        "id": "5hsvqob3HhJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token IDs can be decoded back into strings, for example:"
      ],
      "metadata": {
        "id": "Z2UJydk3M0SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "for id, token in zip(inputs['input_ids'][0], tokens):\n",
        "    print(f\"{id.item():5d} → {token}\")"
      ],
      "metadata": {
        "id": "DZ5aAzLIMv5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whRaY6yOUzSd"
      },
      "source": [
        "Now we can pass the tensor output of the tokenizer through the model, getting its hidden states:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1msn3ZhQ6Cv"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = distilbert(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz4UyRT7Hq8G"
      },
      "source": [
        "Above, ```torch.no_grad()``` guarantees that no gradients are computed in this code block. So the model's weights cannot be updated. This is what we want now: obtain sentence pair vectors from the model, without making any changes to the model itself.\n",
        "\n",
        "You should use ```output.last_hidden_state``` that stores the last hidden layer. From that, you only need the first vector, which corresponds to the [CLS] token. It is the first in the sequence so has index 0.\n",
        "\n",
        "**Exercise**. What is the size of that vector? Check!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUzkfHglRBIv"
      },
      "outputs": [],
      "source": [
        "#your code\n",
        "print(len(output.last_hidden_state[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBOKubHp1Qhi"
      },
      "source": [
        "Now we can define linear regression model in pyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1AxCRLH1Qho"
      },
      "outputs": [],
      "source": [
        "class Regression(torch.nn.Module):\n",
        "  def __init__(self, input_dim,output_dim):\n",
        "    super(Regression,self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim,output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    outputs = self.linear(x)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19SEdasaU8pS"
      },
      "source": [
        "It is the last vector of the [CLS] token that is normally used for sequence classification with BERT models. Train and test a logistic regression classifier on top of (frozen) DistilBERT embeddings for sentence pairs in SICK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wUo0J_FPfr5"
      },
      "outputs": [],
      "source": [
        "distilbert_embdim = distilbert.config.hidden_size\n",
        "\n",
        "entailment_model = Regression(distilbert_embdim,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Define loss and initialize an optimizer:"
      ],
      "metadata": {
        "id": "xQu_BD_m5ROD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(entailment_model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "sRMboxJ45Qkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4yKLr-mPAMB"
      },
      "source": [
        "Now prepare the data: convert ```sick_train_examples``` and ```sick_test_examples``` into lists of torch tesors (embeddings of the sentence pairs).\n",
        "\n",
        "__Hint__: to speed up neural computation, it can be helpful to move data and model to the GPU, a special processor that handles numeric tensor operations more efficiently (you may use commands like ```model.to('cuda')```, ```tensordata.to('cuda')```). For further non-neural computation, data can be moved back to the CPU ```tensordata.to('cpu')```. In case you (temporarily) don't have access to a GPU, you can use `'cpu'` instead of `'cuda'` to do the computation on a CPU. However, this will slow the computation down.\n",
        "\n",
        "Processing all the sentence pairs in pretrained DistilBERT will take a couple of minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8lSYoD3PA-v"
      },
      "outputs": [],
      "source": [
        "#enabling GPU computation\n",
        "distilbert.to('cuda')\n",
        "\n",
        "sick_train_examples = [tokenizer(sentencePair[0],sentencePair[1], return_tensors=\"pt\") for sentencePair in sick_train_examples]\n",
        "sick_test_examples = [tokenizer(sentencePair[0],sentencePair[1], return_tensors=\"pt\") for sentencePair in sick_test_examples]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, convert ```sick_train_labels``` and ```sick_test_labels``` into lists of indices:"
      ],
      "metadata": {
        "id": "qCS7LpM_5yBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDxP2-K_WKpy"
      },
      "outputs": [],
      "source": [
        "sick_train_labels_idx= [i for i in range(len(sick_train_labels))]\n",
        "sick_test_labels_idx= [i for i in range(len(sick_test_labels))]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Train for 40 epochs and test a regression model on DistilBERT embeddings to classify SICK with the three entailment labels. Print out train and test accuracy and loss every 5 epochs."
      ],
      "metadata": {
        "id": "p-i1Gwpc6FUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFpESTU_Gz7P"
      },
      "outputs": [],
      "source": [
        "epochs=40\n",
        "#accuracy\n",
        "def accuracy(predictions, scores):\n",
        "  score = 0\n",
        "  for i in range(0,len(predictions)):\n",
        "    if predictions[i] == scores[i]:\n",
        "      score +=1\n",
        "  return score / len(scores)\n",
        "\n",
        "# training model\n",
        "for epoch in trange(epochs):\n",
        "\n",
        "  if(epoch+1) % 5 ==0:\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwQdUS8-VVGY"
      },
      "source": [
        "##4.1.3 Fine-tuning\n",
        "\n",
        "Can you improve the performance of entailment classification even further?\n",
        "\n",
        "_Fine-tune_ DistilBERT on the task: train and test a logistic regression classifier on top of DistilBERT embeddings for sentence pairs in SICK while updating the model weights. Define a new model class that combines DistilBERT and regression models into a single model, which passes sentence pair input through DistilBERT and uses the output embedding of the CLS token as input to regression, which finally produces the output of the whole combined model.\n",
        "\n",
        "The weights of this bigger model (i.e. both DistilBERT weights and regression weights) can then be updated by the optimizer.\n",
        "\n",
        "**Exercise**. Train the whole pipeline for 4 epochs using the train/test split as above.\n",
        "\n",
        "For this last exercise, using a GPU is essential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czdFkcOcVm5Y"
      },
      "outputs": [],
      "source": [
        "epochs = 4\n",
        "#your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 Autoregressive Transformer (GPT2)"
      ],
      "metadata": {
        "id": "cPM31NVnN2cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2.1 Text generation with GPT2\n"
      ],
      "metadata": {
        "id": "xEG4PBS339kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load and use the smallest version of GPT2 to save time and resources; it suffices for our purposes."
      ],
      "metadata": {
        "id": "fK6JDVdQakmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "hXo_t8-dc9qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To put GPT-2 to a test, we can create a list of examples as text snippets that GPT-2 is then asked to complete. We use three simple prompts here, taken from the beginning of NL Times articles in 2024."
      ],
      "metadata": {
        "id": "B9it3ASiZqal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts=[\n",
        "    \"The Schoof I Cabinet will likely have fewer Ministers and State Secretaries than the outgoing Rutte IV Cabinet, sources close to the formation process told the Telegraaf.\",\n",
        "    \"Outgoing Agriculture Minister Piet Adema wants to ban business of yoga sessions that also involve puppies. Puppy yoga has been offered in Amsterdam for months now.\",\n",
        "    \"The Walt Disney Company will participate in the Pride boat parade in the Netherlands for the first time on Saturday. \"]"
      ],
      "metadata": {
        "id": "eQ0v4S64Xn8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Define functions ```run_on_prompt``` and ```run_on_prompts```. ```run_on_prompt``` takes a sting prompt and returns a list of n continuations of the prompt. ```run_on_prompts``` that takes a list of prompts and returns a list of lists of n continuations of each prompt.\n",
        "Schematically, `run_on_prompts([prompt1,prompt2],nsamples=2)` should output `[[continuation1OfPrompt1,continuation2OfPrompt1],[continuation1OfPrompt2,continuation1OfPrompt2]]`.\n",
        "\n",
        "*  Make sure your functions contain the `temperature` and `top_p` sampling parameters as you will be asked to experiment with them.\n",
        "*  Your ```run_on_prompt``` function should also print the prompt and generated text continuations so you can inspect them.\n",
        "*  Make sure that the outputs produced by ```run_on_prompt``` do not contain the prompt itself.\n",
        "\n",
        "**Hints:**\n",
        "* You'll need to use both the tokenizer and the model's generate() method. Look up the necessary details in the Hugging Face text [generation documentation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation) and the [reference on generation strategies](https://huggingface.co/docs/transformers/generation_strategies).\n",
        "* To extract just the continuation, compare the length of the original prompt to the generated text."
      ],
      "metadata": {
        "id": "oq3V_aT74apP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "def run_on_prompt(model, prompt, nsamples=3, length=1,\n",
        "                   temperature=1,\n",
        "                   top_k=None,\n",
        "                   top_p=1):\n",
        "    \"\"\"\n",
        "    Generates text continuations for a single prompt using a GPT model.\n",
        "\n",
        "    Args:\n",
        "        model: GPT model instance.\n",
        "        prompt: A string prompt.\n",
        "        nsamples: The number of samples to generate for the prompt.\n",
        "        length: The maximum length of new tokens to generate.\n",
        "        temperature: Temperature parameter for controlling randomness.\n",
        "        top_k: Top-k sampling parameter.\n",
        "        top_p: Top-p (nucleus) sampling parameter.\n",
        "\n",
        "    Returns:\n",
        "        A list of generated text continuations (without the prompt).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model.generate(input, max_new_tokens=length, do_sample=True, temperature=temperature, top_k=top_k if top_k is not None else 0, top_p=top_p, num_return_sequences=nsamples, pad_token_id=tokenizer.eos_token_id)\n",
        "    generated = []\n",
        "    for o in output:\n",
        "      generated.append(tokenizer.decode(o[input.shape[-1]:], skip_special_tokens=True))\n",
        "    print(f\"prompt: {prompt}\\n\")\n",
        "    for g in generated:\n",
        "      print(f\"generated: {g}\\n\")\n",
        "    return generated\n",
        "\n",
        "\n",
        "def run_on_prompts(model, prompt_list, nsamples=3, length=1,\n",
        "                   temperature=1,\n",
        "                   top_k=None,\n",
        "                   top_p=1):\n",
        "    \"\"\"\n",
        "    Generates text continuations for a list of prompts using a GPT model.\n",
        "\n",
        "    Args:\n",
        "        model: GPT model instance.\n",
        "        prompt_list: A list of strings, where each string is a prompt.\n",
        "        nsamples: The number of samples to generate for each prompt.\n",
        "        length: The maximum length of new tokens to generate.\n",
        "        temperature: Temperature parameter for controlling randomness.\n",
        "        top_k: Top-k sampling parameter.\n",
        "        top_p: Top-p (nucleus) sampling parameter.\n",
        "\n",
        "    Returns:\n",
        "        A list of lists, where each inner list contains continuations for one prompt.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for p in prompt_list:\n",
        "      output.append(run_on_prompt(model, p, nsamples, length, temperature, top_k, top_p))\n",
        "    return output"
      ],
      "metadata": {
        "id": "wsUIstpReBY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run GPT-2 with your function with different temperature values: 0.4 (low), 0.8,  1 (default) and 2 (high). Extreme values are suggested for pedagogical purposes."
      ],
      "metadata": {
        "id": "qoPvx9I1SO9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gptsmall = model.to(device)\n",
        "temp04outputs=run_on_prompts(gptsmall,prompts, length=100, temperature=0.4)"
      ],
      "metadata": {
        "id": "uMORuh2RDQmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp08outputs=run_on_prompts(gptsmall,prompts, length=100, temperature=0.8)"
      ],
      "metadata": {
        "id": "XO9a_r6DKNEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp1outputs=run_on_prompts(gptsmall,prompts, length=100, temperature=1)"
      ],
      "metadata": {
        "id": "qyZ0Bv-DR7Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp2outputs=run_on_prompts(gptsmall,prompts, length=100, temperature=2.0)"
      ],
      "metadata": {
        "id": "0p67DEzpONLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the generated texts. What are your observations about the role of temperature?\n",
        "\n",
        "**Answer**:\n",
        "At a lower temperature, while the information the model gives us might be neither correct nor logically sound, the model at least generates text that is readable and consists mostly of correct sentences related to the prompt, while at higher temperatures the model starts \"rambling\" without any visible structure and using words lacking relation to the prompt or other words in the sentences."
      ],
      "metadata": {
        "id": "HYX5luA8armM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now keep the temperature at default and run text generation with nucleus sampling (p-sampling) with p at 0.2, 0.3, 0.5, and 0.95."
      ],
      "metadata": {
        "id": "_Ug8EZaeTj0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputsp2=run_on_prompts(gptsmall,prompts, length=100,top_p=0.2)"
      ],
      "metadata": {
        "id": "iP-nrYddJ9AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputsp3=run_on_prompts(gptsmall,prompts, length=100,top_p=0.3)"
      ],
      "metadata": {
        "id": "NynZ_BoXCTZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputsp5=run_on_prompts(gptsmall,prompts, length=100,top_p=0.5)"
      ],
      "metadata": {
        "id": "rF53maw5GSgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputsp95=run_on_prompts(gptsmall,prompts, length=100,top_p=0.95)"
      ],
      "metadata": {
        "id": "qennf_ybCb2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are your observations on the outputs at different values of p?\n",
        "\n",
        "**Answer**\n",
        "at low p values the model is rather unoriginal and seems to first generate a rather safe sentence that is very similar to the prompt and repeat it, while at the other extreme it is quite creative, never sticking to the same subject, involving lots of made up people, but creating relatively (grammatically) correct sentences."
      ],
      "metadata": {
        "id": "0y21AYKtUJ0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2.2 Automatic measuring of text diversity"
      ],
      "metadata": {
        "id": "8L3etr0E_IP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You made some qualitative observartions on the output. Can we also identify quantitative differences between the generated texts? One approach is to measure the fraction of unique fragments (e.g. words or word sequences) in text.\n",
        "\n",
        "Define a function `uniqN` that takes a string `ss` and returns the proportion of unique token n-grams of length `n`. For simplicity, separate the string into words using `split()`."
      ],
      "metadata": {
        "id": "6jNDNy6l_tH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def uniqN(ss,n):\n",
        "  seen = []\n",
        "  ss = ss.split()\n",
        "  for s in ss:\n",
        "    if len(s) == n:\n",
        "      if s not in seen:\n",
        "        seen.append(s)\n",
        "  return len(seen) / len(ss)"
      ],
      "metadata": {
        "id": "2D4iVVLPHWN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define a function `avgrepN` which measures the diversity of text in an input corpus `c` (a list of lists of strings), returning the average `uniqN` value for each string in the corpus for n-grams ranging from length 1 to `maxn`. High values mean the texts are diverse, low values indicate a lot of repeating n-grams."
      ],
      "metadata": {
        "id": "WsmhrkCwXJGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avgrepN(c,maxn=3):\n",
        "  unique = 0;\n",
        "  i = 0;\n",
        "  for n in range(maxn):\n",
        "    for sc in c:\n",
        "      for ss in sc:\n",
        "        unique += uniqN(ss, n + 1)\n",
        "        i += 1\n",
        "  return unique / i"
      ],
      "metadata": {
        "id": "YbqhJwO7JB1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the average diversity for texts generated with diverse values of p under nucleus sampling."
      ],
      "metadata": {
        "id": "UDe9PcDyXvpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(avgrepN(outputsp2))\n",
        "print(avgrepN(outputsp3))\n",
        "print(avgrepN(outputsp5))\n",
        "print(avgrepN(outputsp95))\n"
      ],
      "metadata": {
        "id": "FStVQEL4Jgfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the average diversity for texts generated with diverse values of temperature."
      ],
      "metadata": {
        "id": "I7fm7FsYYkxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(avgrepN(temp04outputs))\n",
        "print(avgrepN(temp08outputs))\n",
        "print(avgrepN(temp1outputs))\n",
        "print(avgrepN(temp2outputs))"
      ],
      "metadata": {
        "id": "QfVfNxYtPCGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compare the GPT2-generated outputs with reference texts, i.e. continuations of the prompts in the actual texts."
      ],
      "metadata": {
        "id": "U45EL_McZEQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REFERENCE1='''These types of lessons involve young dogs running around that the participants can cuddle after the lesson.\n",
        "Adema said he does not believe it is healthy for the young animals. \\\"I don't think that is suitable. Puppies need to sleep. They are at a very early stage of their development,\\\" he stated after the regular weekly Cabinet meeting.\n",
        "\\\"It serves no purpose at all and makes no sense. It really has to stop.\\\" He is preparing a draft proposal of the ban so that his replacement in the next Cabinet can implement it.\n",
        "'''\n",
        "REFERENCE2='''The Schoof I Cabinet will likely have fewer Ministers and State Secretaries than the outgoing Rutte IV Cabinet, sources close to the formation process told the Telegraaf. As the intended Prime Minister Dick Schoof is considered “party-less” and represents all four parties in the coalition, the Cabinet will likely have four Deputy Prime Ministers - one each from the PVV, VVD, NSC, and BBB.\n",
        "Schoof was sparing with information after his first formation session. '\\“It was a beautiful day,\\” he told the press after meeting with the leaders of the coalition parties and formateur Richard van Zwol.\n",
        "'''\n",
        "\n",
        "REFERENCE3='''The employees are taking part in the initiative of a company working group that promotes inclusion.\n",
        "Disney has increasingly focused on inclusion in recent years. Pride Walks have already taken place in London, Berlin, and Paris. However, this is the first time that the company has taken such a clear stand and made such a visible statement during a Dutch Pride. \"We are very pleased with the great interest and diversity of registrations for this year's boat parade,\" said the Utrecht Pride organization. \\\"The selected boats show what Utrecht Pride stands for, making the LGBTIQ+ community visible in all its facets, and we look forward to everyone enjoying this beautiful event, both on the water and along the side.\\\"'''"
      ],
      "metadata": {
        "id": "tpAwKDeVLTrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(avgrepN([[REFERENCE1],[REFERENCE2],[REFERENCE3]]))"
      ],
      "metadata": {
        "id": "Ief3nVB6Lboj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do the reference texts compare to texts generated under different values of p or temperature?\n",
        "\n",
        "**Answer**\n",
        "In terms of diversity as well as \"feel\" when reading the reference texts are the most similar to a temperature of about 1 and a p of about 0.5. When decreasing either the output becomes too safe and unoriginal compared to the reference but use higher values and the model either begins rambling or has trouble producing coherent sentences."
      ],
      "metadata": {
        "id": "FWPyXG26Ze4s"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}